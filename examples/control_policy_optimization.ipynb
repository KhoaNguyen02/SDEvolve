{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import gymnax\n",
    "\n",
    "from kozax.genetic_programming import GeneticProgramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymFitnessFunction:\n",
    "    def __init__(self, env_name) -> None:\n",
    "        self.env, self.env_params = gymnax.make(env_name)\n",
    "        self.num_steps = 200\n",
    "\n",
    "    def __call__(self, candidate, keys, tree_evaluator):\n",
    "        reward = jax.vmap(self.simulate_rollout, in_axes=(None, 0, None))(candidate, keys, tree_evaluator)\n",
    "        return jnp.mean(reward)\n",
    "        \n",
    "    def simulate_rollout(self, candidate, key, tree_evaluator):\n",
    "        key, subkey = jr.split(key)\n",
    "        state, env_state = self.env.reset(subkey, self.env_params)\n",
    "\n",
    "        def policy(state):\n",
    "            a = tree_evaluator(candidate, state)[0]\n",
    "            return jax.lax.select(a == 0, 1, jax.lax.select(a > 0, 2, 0))\n",
    "\n",
    "        def step_fn(carry, _):\n",
    "            state, env_state, key = carry\n",
    "\n",
    "            action = policy(state)\n",
    "\n",
    "            key, subkey = jr.split(key)\n",
    "            next_state, next_env_state, reward, _, _ = self.env.step(\n",
    "                subkey, env_state, action, self.env_params\n",
    "            )\n",
    "\n",
    "            return (next_state, next_env_state, key), (state, reward)\n",
    "\n",
    "        (_, (states, rewards)) = jax.lax.scan(\n",
    "            step_fn, (state, env_state, key), None, length=self.num_steps\n",
    "        )\n",
    "        \n",
    "        first_success = jnp.argmax(rewards)\n",
    "        return (first_success + (first_success == 0) * self.num_steps)/self.num_steps\n",
    "    \n",
    "fitness_function = GymFitnessFunction(\"Acrobot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "population_size = 100\n",
    "num_populations = 10\n",
    "num_generations = 50\n",
    "batch_size = 4\n",
    "\n",
    "#Define operators and variables\n",
    "operator_list = [\n",
    "    (\"+\", lambda x, y: jnp.add(x, y), 2, 0.5), \n",
    "    (\"-\", lambda x, y: jnp.subtract(x, y), 2, 0.1), \n",
    "    (\"*\", lambda x, y: jnp.multiply(x, y), 2, 0.5), \n",
    "    (\"**\", lambda x, y: jnp.power(x, y), 2, 0.1), \n",
    "    (\"/\", lambda x, y: jnp.divide(x, y), 2, 0.1),\n",
    "    (\"sin\", lambda x: jnp.sin(x), 1, 0.1)\n",
    "    ]\n",
    "\n",
    "variable_list = [[\"y1\", \"y2\", \"y3\", \"y4\", \"y5\", \"y6\"]]\n",
    "\n",
    "#Initialize strategy\n",
    "strategy = GeneticProgramming(num_generations, population_size, fitness_function, operator_list, variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "key, init_key, data_key = jr.split(key, 3)\n",
    "\n",
    "batch_keys = jr.split(data_key, batch_size)\n",
    "\n",
    "#Initial population\n",
    "population = strategy.initialize_population(init_key)\n",
    "\n",
    "for g in range(num_generations):\n",
    "    key, eval_key, sample_key = jr.split(key, 3)\n",
    "    fitness, population = strategy.evaluate_population(population, (batch_keys), eval_key)\n",
    "\n",
    "    if g < (num_generations-1):\n",
    "        population = strategy.evolve(population, fitness, sample_key)\n",
    "\n",
    "strategy.print_pareto_front()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
