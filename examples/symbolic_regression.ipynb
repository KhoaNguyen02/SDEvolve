{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import diffrax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import diffrax\n",
    "\n",
    "from jax import Array\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "from kozax.genetic_programming import GeneticProgramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LotkaVolterra():\n",
    "    def __init__(self):\n",
    "        self.n_var = 2\n",
    "\n",
    "        self.init_mu = jnp.array([10, 10])\n",
    "        self.init_sd = 2\n",
    "\n",
    "        self.alpha = 1.1\n",
    "        self.beta = 0.4\n",
    "        self.delta = 0.1\n",
    "        self.gamma = 0.4\n",
    "\n",
    "    def sample_init_states(self, batch_size, key):\n",
    "        return jr.uniform(key, shape = (batch_size,self.n_var), minval=5, maxval=15)\n",
    "    \n",
    "    def drift(self, t, state, args):\n",
    "        return jnp.array([self.alpha * state[0] - self.beta * state[0] * state[1], self.delta * state[0] * state[1] - self.gamma * state[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"Evaluator for candidates on symbolic regression tasks\n",
    "\n",
    "    Attributes:\n",
    "        dt0: Initial step size for integration\n",
    "        fitness_function: Function that computes the fitness of a candidate\n",
    "        system: ODE term of the drift function\n",
    "        solver: Solver used for integration\n",
    "        stepsize_controller: Controller for the stepsize during integration\n",
    "        max_steps: The maximum number of steps that can be used in integration\n",
    "    \"\"\"\n",
    "    def __init__(self, solver: diffrax.AbstractSolver = diffrax.Euler(), dt0: float = 0.01, max_steps: int = 16**4, stepsize_controller: diffrax.AbstractStepSizeController = diffrax.ConstantStepSize(), optimize_dimensions: Array = None) -> None:\n",
    "        self.dt0 = dt0\n",
    "        if optimize_dimensions is None:\n",
    "            self.fitness_function = lambda pred_ys, true_ys: jnp.mean(jnp.sum(jnp.abs(pred_ys-true_ys), axis=-1))/jnp.mean(true_ys) #Mean Absolute Error\n",
    "        else:\n",
    "            if len(optimize_dimensions) > 1:\n",
    "                self.fitness_function = lambda pred_ys, true_ys: jnp.mean(jnp.sum(jnp.abs(pred_ys[:,optimize_dimensions]-true_ys[:,optimize_dimensions]), axis=-1))/jnp.mean(true_ys[:,optimize_dimensions])\n",
    "            else:\n",
    "                self.fitness_function = lambda pred_ys, true_ys: jnp.mean(jnp.abs(pred_ys[:,optimize_dimensions]-true_ys[:,optimize_dimensions]))/jnp.mean(true_ys[:,optimize_dimensions])\n",
    "\n",
    "        self.system = diffrax.ODETerm(self._drift)\n",
    "        self.solver = solver\n",
    "        self.stepsize_controller = stepsize_controller\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def __call__(self, candidate, data: Tuple, tree_evaluator: Callable) -> float:\n",
    "        \"\"\"Evaluates the candidate on a task\n",
    "\n",
    "        :param coefficients: The coefficients of the candidate\n",
    "        :param nodes: The nodes and index references of the candidate\n",
    "        :param data: The data required to evaluate the candidate\n",
    "        :param tree_evaluator: Function for evaluating trees\n",
    "\n",
    "        Returns: Fitness of the candidate\n",
    "        \"\"\"\n",
    "        fitness, _ = self.evaluate_candidate(candidate, data, tree_evaluator)\n",
    "\n",
    "        return jnp.mean(fitness)\n",
    "    \n",
    "    def evaluate_candidate(self, candidate: Array, data: Tuple, tree_evaluator: Callable) -> Tuple[Array, float]:\n",
    "        \"\"\"Evaluates a candidate given a task and data\n",
    "\n",
    "        :param candidate: Candidate that is evaluated\n",
    "        :param data: The data required to evaluate the candidate\n",
    "        \n",
    "        Returns: Predictions and fitness of the candidate\n",
    "        \"\"\"\n",
    "        return jax.vmap(self.evaluate_time_series, in_axes=[None, 0, None, 0, None])(candidate, *data, tree_evaluator)\n",
    "    \n",
    "    def evaluate_time_series(self, candidate: Array, x0: Array, ts: Array, ys: Array, tree_evaluator: Callable) -> Tuple[Array, float]:\n",
    "        \"\"\"Solves the candidate as a differential equation and returns the predictions and fitness\n",
    "\n",
    "        :param candidate: Candidate that is evaluated\n",
    "        :param x0: Initial conditions of the environment\n",
    "        :param ts: Timepoints of which the system has to be solved\n",
    "        :param ys: Ground truth data used to compute the fitness\n",
    "        :param process_noise_key: Key to generate process noise\n",
    "        :param tree_evaluator: Function for evaluating trees\n",
    "        \n",
    "        Returns: Predictions and fitness of the candidate\n",
    "        \"\"\"\n",
    "        \n",
    "        saveat = diffrax.SaveAt(ts=ts)\n",
    "        event_nan = diffrax.Event(self.cond_fn_nan)\n",
    "\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            self.system, self.solver, ts[0], ts[-1], self.dt0, x0, args=(candidate, tree_evaluator), saveat=saveat, max_steps=self.max_steps, stepsize_controller=self.stepsize_controller, \n",
    "            adjoint=diffrax.DirectAdjoint(), throw=False, event=event_nan\n",
    "        )\n",
    "        pred_ys = sol.ys\n",
    "        fitness = self.fitness_function(pred_ys, ys) + 1.0*jnp.mean(jnp.where(pred_ys<0, jnp.abs(pred_ys), 0))\n",
    "\n",
    "        return fitness, pred_ys\n",
    "    \n",
    "    def _drift(self, t, x, args):\n",
    "        candidate, tree_evaluator = args\n",
    "\n",
    "        dx = tree_evaluator(candidate, x)\n",
    "        return dx\n",
    "    \n",
    "    def cond_fn_nan(self, t, y, args, **kwargs):\n",
    "        return jnp.where(jnp.any(jnp.isinf(y) + jnp.isnan(y)), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(key, env, dt, T, batch_size=20):\n",
    "    x0s = env.sample_init_states(batch_size, key)\n",
    "    ts = jnp.arange(0, T, dt)\n",
    "\n",
    "    def solve(env, ts, x0):\n",
    "        solver = diffrax.Dopri5()\n",
    "        dt0 = 0.001\n",
    "        saveat = diffrax.SaveAt(ts=ts)\n",
    "\n",
    "        system = diffrax.ODETerm(env.drift)\n",
    "\n",
    "        sol = diffrax.diffeqsolve(system, solver, ts[0], ts[-1], dt0, x0, saveat=saveat, max_steps=2000, \n",
    "                                  adjoint=diffrax.DirectAdjoint(), stepsize_controller=diffrax.PIDController(atol=1e-7, rtol=1e-7, dtmin=0.001))\n",
    "        \n",
    "        return sol.ys\n",
    "\n",
    "    ys = jax.vmap(solve, in_axes=[None, None, 0])(env, ts, x0s)\n",
    "    \n",
    "    return x0s, ts, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30\n",
    "dt = 0.2\n",
    "env = LotkaVolterra()\n",
    "\n",
    "operator_list = [\n",
    "        (\"+\", lambda x, y: jnp.add(x, y), 2, 0.5), \n",
    "        (\"-\", lambda x, y: jnp.subtract(x, y), 2, 0.1), \n",
    "        (\"*\", lambda x, y: jnp.multiply(x, y), 2, 0.5), \n",
    "        (\"**\", lambda x, y: jnp.power(x, y), 2, 0.1), \n",
    "        (\"/\", lambda x, y: jnp.divide(x, y), 2, 0.1)\n",
    "    ]\n",
    "\n",
    "variable_list = [[\"x\" + str(i) for i in range(env.n_var)]]\n",
    "\n",
    "population_size = 200\n",
    "num_populations = 10\n",
    "num_generations = 150\n",
    "\n",
    "fitness_function = Evaluator(solver=diffrax.Dopri5(), dt0 = 0.01, stepsize_controller=diffrax.PIDController(atol=1e-6, rtol=1e-6, dtmin=0.001), max_steps=300, optimize_dimensions = jnp.array([0]))\n",
    "\n",
    "layer_sizes = jnp.array([env.n_var])\n",
    "\n",
    "strategy = GeneticProgramming(num_generations, population_size, fitness_function, operator_list, variable_list, layer_sizes, num_populations = num_populations,\n",
    "                        max_nodes = 15, migration_period=5, coefficient_optimisation=\"ES\", ES_n_offspring = 20, ES_n_iterations = 1, size_parsimony=0.003, \n",
    "                        optimise_coefficients_elite=100, init_learning_rate=0.1)\n",
    "\n",
    "seeds = jnp.arange(10)\n",
    "\n",
    "for seed in seeds:\n",
    "    strategy.reset()\n",
    "\n",
    "    key = jr.PRNGKey(seed)\n",
    "    key, init_key, data_key = jr.split(key, 3)\n",
    "    x0s, ts, ys = get_data(data_key, env, dt=dt, T=T, batch_size=8)\n",
    "\n",
    "    population = strategy.initialize_population(init_key)\n",
    "\n",
    "    for g in range(num_generations):\n",
    "        key, eval_key, sample_key = jr.split(key, 3)\n",
    "        fitness, population = strategy.evaluate_population(population, (x0s, ts, ys), eval_key)\n",
    "\n",
    "        if g < (num_generations-1):\n",
    "            population = strategy.evolve(population, fitness, sample_key)\n",
    "\n",
    "    strategy.print_pareto_front()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
